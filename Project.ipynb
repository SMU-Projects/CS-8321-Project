{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Final Paper\n",
    "\n",
    "#### CS8321: Neural Networks and Machine Learning\n",
    "    \n",
    "Will Lacey - 45906124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "<em>My description ya ya ya</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Initialization\n",
    "\n",
    "Before we begin, let's import essential packages for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import multiprocessing.pool\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import scipy.misc\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "from PIL import Image\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input, UpSampling2D\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the VGG Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(inputs, target_layer):\n",
    "    \"\"\"\n",
    "    Loads the layers of the VGG network\n",
    "    \"\"\"\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    \"\"\"\n",
    "    Loads the VGG weights from Chollet's github. Sets the following weights to the convolutional layers. \n",
    "    \"\"\"\n",
    "    \n",
    "    link_p1 = 'https://github.com/fchollet/deep-learning-models/releases/'\n",
    "    link_p2 = 'download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    \n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                            (link_p1+link_p2),\n",
    "                            cache_subdir='models',\n",
    "                            file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    f = h5py.File(weights_path)\n",
    "    \n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            g = f[b_name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.);\n",
    "        Prepares the VGG model\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19')\n",
    "    load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    \"\"\"\n",
    "    Preprocesses input for the model; Most notably this function transfers the RGB of an image to BGR\n",
    "    \"\"\"\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - np.array([103.939, 116.779, 123.68])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    \"\"\"\n",
    "    Gets a set of decoder layers for a decoder model; this structure will for the most part mirror \n",
    "        VGG's implementation\n",
    "    \"\"\"\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "    if layer == 1:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    if layer == 2:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "    if layer == 3:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    if layer == 4:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    if layer == 5:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(x):\n",
    "    \"\"\"\n",
    "    Helper loss function using L2 normalization\n",
    "    \"\"\"\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None, loss_lambda=1):\n",
    "        \"\"\"\n",
    "        Initialization function for the EncoderDecoder Object\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        self.loss_lambda = loss_lambda\n",
    "\n",
    "        # Builds the model from the encoder and decoder functions seen above\n",
    "        self.encoder = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path, compile=False) # load pre-trained weights for decoder\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer) # ...or build weights froms scratch\n",
    "\n",
    "        # models are sequentially constructed\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "\n",
    "        self.loss = self.create_loss_fn(self.encoder)\n",
    "\n",
    "        # using adam optimizer and our defined loss\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, beta_1=0.4, beta_2=0.999)\n",
    "        self.model.compile(optimizer=optimizer, loss=self.loss)\n",
    "\n",
    "    def create_loss_fn(self, encoder):\n",
    "        def get_encodings(inputs):\n",
    "            \"\"\"\n",
    "            Gets the VGG encodings \n",
    "            \"\"\"\n",
    "            encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            \"\"\"\n",
    "            Defined loss function for training\n",
    "            \"\"\"\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                   self.loss_lambda*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        \"\"\"\n",
    "        Creates the decoder submodel from the decoder functions above\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "\n",
    "    def export_decoder(self):\n",
    "        \"\"\"\n",
    "        Exports the decoder weights/model\n",
    "        \"\"\"\n",
    "        self.decoder.save('data/models/decoder_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_images(path):\n",
    "    \"\"\"\n",
    "    Counts the number of files within a directory; the structure of the directory must be \n",
    "        dataset -> class directories -> class images\n",
    "    \"\"\"\n",
    "    white_list_file_types = ['png', 'jpg', 'jpeg', 'bmp', 'ppm']\n",
    "    num_samples = 0\n",
    "    \n",
    "    path, directories, files = next(os.walk(path))\n",
    "    for directory in directories:\n",
    "        path_of_dir = path + '/' + directory\n",
    "        p, ds, fs = next(os.walk(path_of_dir))\n",
    "        for f in fs:\n",
    "            for file_type in white_list_file_types: \n",
    "                if file_type in f:\n",
    "                    num_samples += 1\n",
    "    return num_samples\n",
    "\n",
    "def format_image(img, is_clipping=False): \n",
    "    \"\"\"\n",
    "    Formats an image by either clipping values below or above 0 and 255 or alternatively \n",
    "        normalizes the least and the greatest values relative to 0 and 255; float image is \n",
    "        returned as a uint8\n",
    "    \"\"\"\n",
    "    if is_clipping:\n",
    "        img = np.clip(img / 255, 0, 1)\n",
    "        \n",
    "    else:\n",
    "        if np.amin(img) < 0:\n",
    "            adjust = - np.amin(img)\n",
    "            img = img + adjust\n",
    "        if np.amax(img) > 255:\n",
    "            img = img.astype(np.float64) / np.amax(img) \n",
    "        else:\n",
    "            img = img.astype(np.float64) / 255\n",
    "        \n",
    "    img = 255 * img # Now scale by 255\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    \"\"\"\n",
    "    Generates data from an image directory\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "class OutputPreview(Callback):\n",
    "\n",
    "    def __init__(self, model, test_img_path, preview_dir_path, image_size=(256, 256), increment=5000):\n",
    "        test_img = image.load_img(test_img_path)\n",
    "        \"\"\"\n",
    "        Initialization function for Output Preview Object; Declares class variables\n",
    "        \"\"\"\n",
    "        test_img = test_img.resize(image_size) # Assumes using 3 channels\n",
    "    \n",
    "        test_target = image.img_to_array(test_img)\n",
    "        test_target = np.expand_dims(test_target, axis=0)\n",
    "        self.test_img = test_target\n",
    "        self.model = model\n",
    "\n",
    "        self.preview_dir_path = preview_dir_path\n",
    "\n",
    "        self.increment = increment\n",
    "        self.iteration = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        \"\"\"\n",
    "        Defines what to do during training at the end of a batch; outputs current progress of image reconstruction\n",
    "        \"\"\"\n",
    "        if (self.iteration % self.increment == 0):\n",
    "            output_img = self.model.predict(self.test_img)[0]\n",
    "            fname = '%d.jpg' % self.iteration\n",
    "            out_path = os.path.join(self.preview_dir_path, fname)\n",
    "            \n",
    "            # normalize and convert image then save image to show preview of output\n",
    "            output_img = format_image(output_img)\n",
    "            imageio.imwrite(out_path, output_img)\n",
    "\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Everything is ready for creating our model and training. Now we're going to quickly create some variables to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "train_path = 'data/datasets/faces'\n",
    "image_size = (64, 64)\n",
    "batch_size = 32\n",
    "epochs = 6000\n",
    "\n",
    "target_layer = 5 # ranges from values 1 through 5\n",
    "loss_lambda = 1 # ranges from values 0 through 1\n",
    "\n",
    "is_using_callbacks = True\n",
    "callbacks_image_path = \"data/input/nick_cage.png\"\n",
    "callbacks_preview_path = \"data/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 449 images belonging to 1 classes.\n",
      "Epoch 1/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 14274968832.0000\n",
      "Epoch 2/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 12684830208.0000\n",
      "Epoch 3/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 10966016768.0000\n",
      "Epoch 4/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 11266589696.0000\n",
      "Epoch 5/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 10069912576.0000\n",
      "Epoch 6/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 8376120448.0000\n",
      "Epoch 7/6000\n",
      "4/4 [==============================] - 22s 6s/step - loss: 8958058496.0000\n",
      "Epoch 8/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 8055772288.0000\n",
      "Epoch 9/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 7993358208.0000\n",
      "Epoch 10/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 8171199104.0000\n",
      "Epoch 11/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 7861482880.0000\n",
      "Epoch 12/6000\n",
      "4/4 [==============================] - 22s 6s/step - loss: 7543121408.0000\n",
      "Epoch 13/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 7187100160.0000\n",
      "Epoch 14/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 6893528320.0000\n",
      "Epoch 15/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 7003661696.0000\n",
      "Epoch 16/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 7396412928.0000\n",
      "Epoch 17/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 6561192576.0000\n",
      "Epoch 18/6000\n",
      "4/4 [==============================] - 22s 5s/step - loss: 6483045120.0000\n",
      "Epoch 19/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 5918715776.0000\n",
      "Epoch 20/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5947971072.0000\n",
      "Epoch 21/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5760548096.0000\n",
      "Epoch 22/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5663418240.0000\n",
      "Epoch 23/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 5493258752.0000\n",
      "Epoch 24/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5632440832.0000\n",
      "Epoch 25/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5337291136.0000\n",
      "Epoch 26/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5078341376.0000\n",
      "Epoch 27/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4908332672.0000\n",
      "Epoch 28/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5454112896.0000\n",
      "Epoch 29/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4835177792.0000\n",
      "Epoch 30/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4446074368.0000\n",
      "Epoch 31/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4988280064.0000\n",
      "Epoch 32/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 5084525696.0000\n",
      "Epoch 33/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4481234112.0000\n",
      "Epoch 34/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4252334080.0000\n",
      "Epoch 35/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4492542848.0000\n",
      "Epoch 36/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4166580288.0000\n",
      "Epoch 37/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4099798848.0000\n",
      "Epoch 38/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4312900992.0000\n",
      "Epoch 39/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3928198272.0000\n",
      "Epoch 40/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3764834240.0000\n",
      "Epoch 41/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4172991872.0000\n",
      "Epoch 42/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 4016824320.0000\n",
      "Epoch 43/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3951669120.0000\n",
      "Epoch 44/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3603567168.0000\n",
      "Epoch 45/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3712406272.0000\n",
      "Epoch 46/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3557560768.0000\n",
      "Epoch 47/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3814755648.0000\n",
      "Epoch 48/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3746226112.0000\n",
      "Epoch 49/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3517753984.0000\n",
      "Epoch 50/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 3215322752.0000\n",
      "Epoch 51/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3341605248.0000\n",
      "Epoch 52/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3348936448.0000\n",
      "Epoch 53/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3267102912.0000\n",
      "Epoch 54/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3334281920.0000\n",
      "Epoch 55/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3450414976.0000\n",
      "Epoch 56/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3083612416.0000\n",
      "Epoch 57/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3184365632.0000\n",
      "Epoch 58/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3056190144.0000\n",
      "Epoch 59/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3152123072.0000\n",
      "Epoch 60/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3075514432.0000\n",
      "Epoch 61/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3154833024.0000\n",
      "Epoch 62/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2887160640.0000\n",
      "Epoch 63/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3043176384.0000\n",
      "Epoch 64/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3164525184.0000\n",
      "Epoch 65/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3225905856.0000\n",
      "Epoch 66/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3154860224.0000\n",
      "Epoch 67/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2829781440.0000\n",
      "Epoch 68/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2838737152.0000\n",
      "Epoch 69/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 2644432192.0000\n",
      "Epoch 70/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3069123264.0000\n",
      "Epoch 71/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2984572032.0000\n",
      "Epoch 72/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2914989568.0000\n",
      "Epoch 73/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2740756288.0000\n",
      "Epoch 74/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2684236672.0000\n",
      "Epoch 75/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2493741568.0000\n",
      "Epoch 76/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2537487232.0000\n",
      "Epoch 77/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2530147776.0000\n",
      "Epoch 78/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2927494144.0000\n",
      "Epoch 79/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 3228157952.0000\n",
      "Epoch 80/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2884522048.0000\n",
      "Epoch 81/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2519033856.0000\n",
      "Epoch 82/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2550285824.0000\n",
      "Epoch 83/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2566322048.0000\n",
      "Epoch 84/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2280216896.0000\n",
      "Epoch 85/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2326189504.0000\n",
      "Epoch 86/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2564278592.0000\n",
      "Epoch 87/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2502712704.0000\n",
      "Epoch 88/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2615639680.0000\n",
      "Epoch 89/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2426521536.0000\n",
      "Epoch 90/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2319699456.0000\n",
      "Epoch 91/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2373525440.0000\n",
      "Epoch 92/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2322811456.0000\n",
      "Epoch 93/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2270905792.0000\n",
      "Epoch 94/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2296513344.0000\n",
      "Epoch 95/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2471341248.0000\n",
      "Epoch 96/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2405229376.0000\n",
      "Epoch 97/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2285896928.0000\n",
      "Epoch 98/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2172805600.0000\n",
      "Epoch 99/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2391395776.0000\n",
      "Epoch 100/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2690141312.0000\n",
      "Epoch 101/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2174493408.0000\n",
      "Epoch 102/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2181472096.0000\n",
      "Epoch 103/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2289955968.0000\n",
      "Epoch 104/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2405710144.0000\n",
      "Epoch 105/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2081734016.0000\n",
      "Epoch 106/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2134242176.0000\n",
      "Epoch 107/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2595051104.0000\n",
      "Epoch 108/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2338819392.0000\n",
      "Epoch 109/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2176794112.0000\n",
      "Epoch 110/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1950586784.0000\n",
      "Epoch 111/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2096759584.0000\n",
      "Epoch 112/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2168207616.0000\n",
      "Epoch 113/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1938070048.0000\n",
      "Epoch 114/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2037312544.0000\n",
      "Epoch 115/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2176904576.0000\n",
      "Epoch 116/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1952454848.0000\n",
      "Epoch 117/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1977799264.0000\n",
      "Epoch 118/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2001256832.0000\n",
      "Epoch 119/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2019872608.0000\n",
      "Epoch 120/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2084004064.0000\n",
      "Epoch 121/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2198622304.0000\n",
      "Epoch 122/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1971686912.0000\n",
      "Epoch 123/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2083074336.0000\n",
      "Epoch 124/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1818923744.0000\n",
      "Epoch 125/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1891132032.0000\n",
      "Epoch 126/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1991784480.0000\n",
      "Epoch 127/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2055951936.0000\n",
      "Epoch 128/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2147833664.0000\n",
      "Epoch 129/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1827569376.0000\n",
      "Epoch 130/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1748670560.0000\n",
      "Epoch 131/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1768542336.0000\n",
      "Epoch 132/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1975173088.0000\n",
      "Epoch 133/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 2007264704.0000\n",
      "Epoch 134/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1930226720.0000\n",
      "Epoch 135/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2076796832.0000\n",
      "Epoch 136/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1936710336.0000\n",
      "Epoch 137/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1710295104.0000\n",
      "Epoch 138/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1889340000.0000\n",
      "Epoch 139/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1886671872.0000\n",
      "Epoch 140/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1853926080.0000\n",
      "Epoch 141/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1765258944.0000\n",
      "Epoch 142/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1852236640.0000\n",
      "Epoch 143/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1703051392.0000\n",
      "Epoch 144/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1879370144.0000\n",
      "Epoch 145/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1763564992.0000\n",
      "Epoch 146/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1914704384.0000\n",
      "Epoch 147/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1739323776.0000\n",
      "Epoch 148/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1651370464.0000\n",
      "Epoch 149/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1739687648.0000\n",
      "Epoch 150/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1746199296.0000\n",
      "Epoch 151/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1901207360.0000\n",
      "Epoch 152/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1817370080.0000\n",
      "Epoch 153/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1763241120.0000\n",
      "Epoch 154/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1771803040.0000\n",
      "Epoch 155/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1693494656.0000\n",
      "Epoch 156/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1704914912.0000\n",
      "Epoch 157/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1629847488.0000\n",
      "Epoch 158/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1609657088.0000\n",
      "Epoch 159/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1642615360.0000\n",
      "Epoch 160/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1799220096.0000\n",
      "Epoch 161/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1702901472.0000\n",
      "Epoch 162/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1572795968.0000\n",
      "Epoch 163/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1826498688.0000\n",
      "Epoch 164/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1633784896.0000\n",
      "Epoch 165/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1666425472.0000\n",
      "Epoch 166/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1623953760.0000\n",
      "Epoch 167/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1617455968.0000\n",
      "Epoch 168/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1717295648.0000\n",
      "Epoch 169/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1722642560.0000\n",
      "Epoch 170/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1764304352.0000\n",
      "Epoch 171/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1488575840.0000\n",
      "Epoch 172/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1369358048.0000\n",
      "Epoch 173/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1486900352.0000\n",
      "Epoch 174/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1789480576.0000\n",
      "Epoch 175/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1893633600.0000\n",
      "Epoch 176/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1539010144.0000\n",
      "Epoch 177/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1447266400.0000\n",
      "Epoch 178/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1986087488.0000\n",
      "Epoch 179/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1599069728.0000\n",
      "Epoch 180/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1617056608.0000\n",
      "Epoch 181/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1676055200.0000\n",
      "Epoch 182/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1493184736.0000\n",
      "Epoch 183/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1522726080.0000\n",
      "Epoch 184/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1454461056.0000\n",
      "Epoch 185/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1495922560.0000\n",
      "Epoch 186/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1586107808.0000\n",
      "Epoch 187/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1585663840.0000\n",
      "Epoch 188/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1890998592.0000\n",
      "Epoch 189/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1547448736.0000\n",
      "Epoch 190/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1425875328.0000\n",
      "Epoch 191/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1569621280.0000\n",
      "Epoch 192/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1442129824.0000\n",
      "Epoch 193/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1467355808.0000\n",
      "Epoch 194/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1453024064.0000\n",
      "Epoch 195/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1449987168.0000\n",
      "Epoch 196/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1563449760.0000\n",
      "Epoch 197/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1530002432.0000\n",
      "Epoch 198/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1521918432.0000\n",
      "Epoch 199/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1525328320.0000\n",
      "Epoch 200/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1350412320.0000\n",
      "Epoch 201/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1333043680.0000\n",
      "Epoch 202/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1480990976.0000\n",
      "Epoch 203/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1484624896.0000\n",
      "Epoch 204/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1560841216.0000\n",
      "Epoch 205/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1565320192.0000\n",
      "Epoch 206/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1640410880.0000\n",
      "Epoch 207/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1445588896.0000\n",
      "Epoch 208/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1597735488.0000\n",
      "Epoch 209/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1429211008.0000\n",
      "Epoch 210/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1288847744.0000\n",
      "Epoch 211/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1395472096.0000\n",
      "Epoch 212/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1437678112.0000\n",
      "Epoch 213/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1344687392.0000\n",
      "Epoch 214/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1398324800.0000\n",
      "Epoch 215/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1354461024.0000\n",
      "Epoch 216/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1294794464.0000\n",
      "Epoch 217/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1608749152.0000\n",
      "Epoch 218/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1569217408.0000\n",
      "Epoch 219/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1431400256.0000\n",
      "Epoch 220/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1355164736.0000\n",
      "Epoch 221/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1313548800.0000\n",
      "Epoch 222/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1462759968.0000\n",
      "Epoch 223/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1470134400.0000\n",
      "Epoch 224/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1345183136.0000\n",
      "Epoch 225/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1321400192.0000\n",
      "Epoch 226/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1448498112.0000\n",
      "Epoch 227/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1330028992.0000\n",
      "Epoch 228/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1399749248.0000\n",
      "Epoch 229/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1368401792.0000\n",
      "Epoch 230/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1377831520.0000\n",
      "Epoch 231/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1354334656.0000\n",
      "Epoch 232/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1330605056.0000\n",
      "Epoch 233/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1252996768.0000\n",
      "Epoch 234/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1237783584.0000\n",
      "Epoch 235/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1234043808.0000\n",
      "Epoch 236/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1247512768.0000\n",
      "Epoch 237/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1312382016.0000\n",
      "Epoch 238/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1443690624.0000\n",
      "Epoch 239/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1429328256.0000\n",
      "Epoch 240/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1686430816.0000\n",
      "Epoch 241/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1588789472.0000\n",
      "Epoch 242/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1715579616.0000\n",
      "Epoch 243/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1483378944.0000\n",
      "Epoch 244/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1415164736.0000\n",
      "Epoch 245/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1278185056.0000\n",
      "Epoch 246/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1350680384.0000\n",
      "Epoch 247/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1286965440.0000\n",
      "Epoch 248/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1166032320.0000\n",
      "Epoch 249/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1201281600.0000\n",
      "Epoch 250/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1231440960.0000\n",
      "Epoch 251/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1311167968.0000\n",
      "Epoch 252/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1456468992.0000\n",
      "Epoch 253/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1304581856.0000\n",
      "Epoch 254/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1352899744.0000\n",
      "Epoch 255/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1360045632.0000\n",
      "Epoch 256/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1090472592.0000\n",
      "Epoch 257/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1218819264.0000\n",
      "Epoch 258/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1202456128.0000\n",
      "Epoch 259/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1382085024.0000\n",
      "Epoch 260/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1271992544.0000\n",
      "Epoch 261/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1204568832.0000\n",
      "Epoch 262/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1211039680.0000\n",
      "Epoch 263/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1157503200.0000\n",
      "Epoch 264/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1172693184.0000\n",
      "Epoch 265/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1213303456.0000\n",
      "Epoch 266/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1257614240.0000\n",
      "Epoch 267/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1194864416.0000\n",
      "Epoch 268/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1240571712.0000\n",
      "Epoch 269/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1212634304.0000\n",
      "Epoch 270/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1194990208.0000\n",
      "Epoch 271/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1293852064.0000\n",
      "Epoch 272/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1231129920.0000\n",
      "Epoch 273/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1240991328.0000\n",
      "Epoch 274/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1151180576.0000\n",
      "Epoch 275/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1228289504.0000\n",
      "Epoch 276/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1328352384.0000\n",
      "Epoch 277/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1134298944.0000\n",
      "Epoch 278/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1184487472.0000\n",
      "Epoch 279/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1157336432.0000\n",
      "Epoch 280/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1326069600.0000\n",
      "Epoch 281/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1599014528.0000\n",
      "Epoch 282/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1502629760.0000\n",
      "Epoch 283/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1279652992.0000\n",
      "Epoch 284/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1359476576.0000\n",
      "Epoch 285/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1226793184.0000\n",
      "Epoch 286/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1269883296.0000\n",
      "Epoch 287/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1231831104.0000\n",
      "Epoch 288/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1205062144.0000\n",
      "Epoch 289/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1124556528.0000\n",
      "Epoch 290/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1110233520.0000\n",
      "Epoch 291/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1142003248.0000\n",
      "Epoch 292/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1101001568.0000\n",
      "Epoch 293/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1098581344.0000\n",
      "Epoch 294/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1227244736.0000\n",
      "Epoch 295/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1233969184.0000\n",
      "Epoch 296/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1105493232.0000\n",
      "Epoch 297/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1120986368.0000\n",
      "Epoch 298/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1108731008.0000\n",
      "Epoch 299/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1119193392.0000\n",
      "Epoch 300/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1300443904.0000\n",
      "Epoch 301/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1213514848.0000\n",
      "Epoch 302/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1080514064.0000\n",
      "Epoch 303/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1127159088.0000\n",
      "Epoch 304/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1403187936.0000\n",
      "Epoch 305/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1162248592.0000\n",
      "Epoch 306/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1161121344.0000\n",
      "Epoch 307/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1243429312.0000\n",
      "Epoch 308/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1479408192.0000\n",
      "Epoch 309/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1190400576.0000\n",
      "Epoch 310/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1105612224.0000\n",
      "Epoch 311/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1101062496.0000\n",
      "Epoch 312/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1260866496.0000\n",
      "Epoch 313/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1203949376.0000\n",
      "Epoch 314/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1161304944.0000\n",
      "Epoch 315/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1069184256.0000\n",
      "Epoch 316/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1031798896.0000\n",
      "Epoch 317/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1055003424.0000\n",
      "Epoch 318/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1324980864.0000\n",
      "Epoch 319/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1060236960.0000\n",
      "Epoch 320/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1092990656.0000\n",
      "Epoch 321/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1080454624.0000\n",
      "Epoch 322/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1181001408.0000\n",
      "Epoch 323/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1132512480.0000\n",
      "Epoch 324/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1084354128.0000\n",
      "Epoch 325/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1054262912.0000\n",
      "Epoch 326/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1190105952.0000\n",
      "Epoch 327/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1087131712.0000\n",
      "Epoch 328/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1172412128.0000\n",
      "Epoch 329/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1168974368.0000\n",
      "Epoch 330/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1143334496.0000\n",
      "Epoch 331/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1034494048.0000\n",
      "Epoch 332/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1026980288.0000\n",
      "Epoch 333/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1051540256.0000\n",
      "Epoch 334/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1073337456.0000\n",
      "Epoch 335/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1110464256.0000\n",
      "Epoch 336/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1103290112.0000\n",
      "Epoch 337/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1313435968.0000\n",
      "Epoch 338/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1087747920.0000\n",
      "Epoch 339/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1056265968.0000\n",
      "Epoch 340/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1052728176.0000\n",
      "Epoch 341/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1035018224.0000\n",
      "Epoch 342/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1142415200.0000\n",
      "Epoch 343/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1223211424.0000\n",
      "Epoch 344/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1066190816.0000\n",
      "Epoch 345/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1067282656.0000\n",
      "Epoch 346/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1038468192.0000\n",
      "Epoch 347/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1050468800.0000\n",
      "Epoch 348/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1045328336.0000\n",
      "Epoch 349/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 995607504.0000\n",
      "Epoch 350/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 964472320.0000\n",
      "Epoch 351/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1021974192.0000\n",
      "Epoch 352/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 920657776.0000\n",
      "Epoch 353/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 954009408.0000\n",
      "Epoch 354/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1150251760.0000\n",
      "Epoch 355/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1172752704.0000\n",
      "Epoch 356/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1037115760.0000\n",
      "Epoch 357/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1068833776.0000\n",
      "Epoch 358/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1223335648.0000\n",
      "Epoch 359/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1020901808.0000\n",
      "Epoch 360/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1055457504.0000\n",
      "Epoch 361/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1007182464.0000\n",
      "Epoch 362/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1002766240.0000\n",
      "Epoch 363/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 952323280.0000\n",
      "Epoch 364/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1034129952.0000\n",
      "Epoch 365/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1098090688.0000\n",
      "Epoch 366/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1033563680.0000\n",
      "Epoch 367/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 932588096.0000\n",
      "Epoch 368/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1040010944.0000\n",
      "Epoch 369/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 994242304.0000\n",
      "Epoch 370/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1085614880.0000\n",
      "Epoch 371/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1254148352.0000\n",
      "Epoch 372/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1202925376.0000\n",
      "Epoch 373/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1147950560.0000\n",
      "Epoch 374/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1100874016.0000\n",
      "Epoch 375/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1015435936.0000\n",
      "Epoch 376/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1000788640.0000\n",
      "Epoch 377/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1023879104.0000\n",
      "Epoch 378/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1010663040.0000\n",
      "Epoch 379/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 930459360.0000\n",
      "Epoch 380/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 970058736.0000\n",
      "Epoch 381/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1071843024.0000\n",
      "Epoch 382/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1048723264.0000\n",
      "Epoch 383/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1053983344.0000\n",
      "Epoch 384/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1045405568.0000\n",
      "Epoch 385/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 928233888.0000\n",
      "Epoch 386/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 939974272.0000\n",
      "Epoch 387/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 926028864.0000\n",
      "Epoch 388/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1038865120.0000\n",
      "Epoch 389/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1024956480.0000\n",
      "Epoch 390/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1089392256.0000\n",
      "Epoch 391/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1197146912.0000\n",
      "Epoch 392/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1049179744.0000\n",
      "Epoch 393/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1026728304.0000\n",
      "Epoch 394/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1012194992.0000\n",
      "Epoch 395/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 980462160.0000\n",
      "Epoch 396/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1027703536.0000\n",
      "Epoch 397/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 954428176.0000\n",
      "Epoch 398/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 991283840.0000\n",
      "Epoch 399/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 986556496.0000\n",
      "Epoch 400/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 875024192.0000\n",
      "Epoch 401/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 923140928.0000\n",
      "Epoch 402/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 938470864.0000\n",
      "Epoch 403/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1054033712.0000\n",
      "Epoch 404/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1208116736.0000\n",
      "Epoch 405/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1347488576.0000\n",
      "Epoch 406/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1053416736.0000\n",
      "Epoch 407/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 965971440.0000\n",
      "Epoch 408/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 984756800.0000\n",
      "Epoch 409/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1018566192.0000\n",
      "Epoch 410/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 964176448.0000\n",
      "Epoch 411/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 902111936.0000\n",
      "Epoch 412/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 952870240.0000\n",
      "Epoch 413/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1079496464.0000\n",
      "Epoch 414/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1005555312.0000\n",
      "Epoch 415/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1032644160.0000\n",
      "Epoch 416/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 942334960.0000\n",
      "Epoch 417/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 909323744.0000\n",
      "Epoch 418/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 955851344.0000\n",
      "Epoch 419/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 967060544.0000\n",
      "Epoch 420/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 972744576.0000\n",
      "Epoch 421/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 862725056.0000\n",
      "Epoch 422/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 928072608.0000\n",
      "Epoch 423/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1038545120.0000\n",
      "Epoch 424/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1286716960.0000\n",
      "Epoch 425/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1037220304.0000\n",
      "Epoch 426/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 968761040.0000\n",
      "Epoch 427/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 896730688.0000\n",
      "Epoch 428/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1083228672.0000\n",
      "Epoch 429/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 875003360.0000\n",
      "Epoch 430/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 846803216.0000\n",
      "Epoch 431/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 900205632.0000\n",
      "Epoch 432/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1214165952.0000\n",
      "Epoch 433/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 999859232.0000\n",
      "Epoch 434/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 873250400.0000\n",
      "Epoch 435/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 893697808.0000\n",
      "Epoch 436/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 871234640.0000\n",
      "Epoch 437/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 913236480.0000\n",
      "Epoch 438/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 937935904.0000\n",
      "Epoch 439/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 901573616.0000\n",
      "Epoch 440/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 875621680.0000\n",
      "Epoch 441/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 919267280.0000\n",
      "Epoch 442/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 981996352.0000\n",
      "Epoch 443/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 981508464.0000\n",
      "Epoch 444/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 963774272.0000\n",
      "Epoch 445/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 975771648.0000\n",
      "Epoch 446/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 988283424.0000\n",
      "Epoch 447/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 898562656.0000\n",
      "Epoch 448/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 878152896.0000\n",
      "Epoch 449/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 869261424.0000\n",
      "Epoch 450/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 882036864.0000\n",
      "Epoch 451/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 841740064.0000\n",
      "Epoch 452/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 910863392.0000\n",
      "Epoch 453/6000\n",
      "4/4 [==============================] - 20s 5s/step - loss: 902543072.0000\n",
      "Epoch 454/6000\n",
      "4/4 [==============================] - 21s 5s/step - loss: 885073328.0000\n",
      "Epoch 455/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 924062912.0000\n",
      "Epoch 456/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1103912800.0000\n",
      "Epoch 457/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 883982992.0000\n",
      "Epoch 458/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 910446224.0000\n",
      "Epoch 459/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 908406784.0000\n",
      "Epoch 460/6000\n",
      "4/4 [==============================] - 23s 6s/step - loss: 853165824.0000\n",
      "Epoch 461/6000\n",
      "4/4 [==============================] - 24s 6s/step - loss: 790454848.0000\n",
      "Epoch 462/6000\n",
      "3/4 [=====================>........] - ETA: 5s - loss: 894940949.3333 "
     ]
    }
   ],
   "source": [
    "# Generates data\n",
    "gen = create_gen(train_path, image_size, batch_size)\n",
    "\n",
    "# steps per epoch are determined by the number of samples and batch size\n",
    "num_samples = count_number_of_images(train_path)\n",
    "steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# define our encoder_decoder model\n",
    "encoder_decoder = EncoderDecoder(input_shape=(image_size[0], image_size[1], 3), \n",
    "                                 target_layer=target_layer, \n",
    "                                 loss_lambda=loss_lambda)\n",
    "\n",
    "# wether or not to use callbacks and preview decoded output during training\n",
    "if (is_using_callbacks):\n",
    "    callbacks = [OutputPreview(encoder_decoder, callbacks_image_path, \n",
    "                               callbacks_preview_path, image_size=image_size)]\n",
    "    encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n",
    "                                        epochs=epochs, callbacks=callbacks)\n",
    "else:\n",
    "    encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "\n",
    "# export trained output\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decoded_image_gallery(input_image_path, output_image_dir):\n",
    "    \"\"\"\n",
    "    Plotting image gallery for decoded output from a encoder/decoder model; Plots an original image next to five \n",
    "    reconstructions from different model layers\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    img = image.load_img(input_image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title('original', size=12)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "    for index in range(5):\n",
    "        plt.subplot(2, 3, index + 2)\n",
    "        path = output_image_dir + 'decoder' + str(index+1) + '_output.png'\n",
    "        img = image.load_img(path)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.title('decoder ' + str(index+1), size=12)\n",
    "#         plt.xticks(())\n",
    "#         plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_image_path = \"data/input/nick_cage.png\"\n",
    "output_images_dir = 'data/output/'\n",
    "\n",
    "# Preprocessing\n",
    "input_img = image.load_img(input_image_path)\n",
    "input_img = input_img.resize(image_size)\n",
    "input_img = image.img_to_array(input_img)\n",
    "input_img = np.expand_dims(input_img, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "for target_layer in range(1, 6):   \n",
    "    # Loads weights for decoder\n",
    "    decoder_path = 'data/models/decoder_' + str(target_layer) + '.h5'\n",
    "    \n",
    "    # Builds model through a specified weights path and target layer\n",
    "    encoder_decoder = EncoderDecoder(input_shape=(image_size[0], image_size[1], 3),\n",
    "                                     decoder_path=decoder_path, \n",
    "                                     target_layer=target_layer)\n",
    "\n",
    "    # gets decoded output\n",
    "    output_img = encoder_decoder.model.predict([input_img])[0]\n",
    "\n",
    "    # Convert image to uint8 then saves output\n",
    "    output_img = format_image(output_img)\n",
    "    output_image_path = output_images_dir + 'decoder' + str(target_layer) + '_output.png'\n",
    "    imageio.imwrite(output_image_path, output_img)\n",
    "    \n",
    "plot_decoded_image_gallery(input_image_path, output_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
